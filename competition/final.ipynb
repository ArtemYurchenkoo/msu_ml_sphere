{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from copy import deepcopy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "            count += 1\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "\n",
    "print(len(doc_to_title))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_rus = SnowballStemmer(\"russian\")\n",
    "stemmer_eng = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words([\"russian\", \"english\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_title(title):\n",
    "    title = title.lower()\n",
    "    words = [stemmer_rus.stem(stemmer_eng.stem(word)) for word in re.sub('[^a-zа-я0-9]', ' ', title).split() \n",
    "                 if not word in stop_words]\n",
    "    return [word for word in words if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_to_words(titles_dict):\n",
    "    words_dict = {}\n",
    "    for ID, title in titles_dict.items():\n",
    "        words_dict[ID] = split_title(title)\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=10):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(ID):\n",
    "    html = ''\n",
    "    with open(\"content/{}.dat\".format(ID)) as f:\n",
    "        for line in f:\n",
    "            html += line\n",
    "    soup = BeautifulSoup(html)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.body.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    trytext = split_title(text)\n",
    "    return get_top_n_words(trytext, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "\n",
    "    def __init__(self, doc_to_title):\n",
    "        \n",
    "        self.n_features = 80\n",
    "        doc_to_title[0] = ''\n",
    "        mydoc = deepcopy(doc_to_title)\n",
    "        for i in tqdm(range(1, len(mydoc))):\n",
    "            for item in get_content(i):\n",
    "                mydoc[i] += ' ' + item[0]\n",
    "        self.doc_to_matrix = TfidfVectorizer().fit_transform(\n",
    "            [' '.join(mydoc[i]) for i in range(len(mydoc))])\n",
    "\n",
    "    def cosine(self, group):\n",
    "        n = self.n_features//4\n",
    "        X = np.empty(shape=(group.size, self.n_features), dtype=np.float)\n",
    "        for i, all_dist in enumerate(pairwise_distances(self.doc_to_matrix[group], metric='cosine')):\n",
    "            X[i, :n] = sorted(all_dist)[1:n + 1]\n",
    "        X[:, n:2*n] = np.mean(X[:, :n], axis=0)\n",
    "        X[:, 2*n:3*n] = np.std(X[:, :n], axis=0)\n",
    "        X[:, 3*n:] = np.median(X[:, :n], axis=0)\n",
    "        return X\n",
    "            \n",
    "    def extract(self, file):\n",
    "        df = pd.read_csv(file)\n",
    "        groups = df.groupby('group_id')\n",
    "        X = np.empty(shape=(df.shape[0], self.n_features), dtype=np.float)\n",
    "        if 'target' in df.columns:\n",
    "            y = np.empty(shape=(df.shape[0], ), dtype=bool)\n",
    "            i = 0\n",
    "            for group_id, group_idx in groups.groups.items():\n",
    "                j = i + group_idx.size\n",
    "                group = df.iloc[group_idx]\n",
    "                y[i:j] = group.target\n",
    "                X[i:j] = self.cosine(group.doc_id)\n",
    "                i = j\n",
    "\n",
    "            return X, y\n",
    "        else:\n",
    "            i = 0\n",
    "            for group_id, group_idx in groups.groups.items():\n",
    "                j = i + group_idx.size\n",
    "                group = df.iloc[group_idx]\n",
    "                X[i:j] = self.cosine(group.doc_id)\n",
    "                i = j\n",
    "\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = titles_to_words(doc_to_title)\n",
    "extractor = Extractor(vec)\n",
    "X_train, y_train = extractor.extract('train_groups.csv')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test, pair_ids = extractor.extract('test_groups.csv')\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1, 'objective': 'binary', 'num_iterations': 500,\n",
    "        'max_bin': 30, 'num_leaves': 7, 'max_depth': 20, 'boosting': 'dart'}\n",
    "thres = np.linspace(0.2, 0.5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tres_scores = np.empty((129, 30))\n",
    "tres_lgb = np.empty((129, 30))\n",
    "tres_log = np.empty((129, 30))\n",
    "tres_trees = np.empty((129, 30))\n",
    "for i in range(1, 130):\n",
    "    ind_test = np.where(groups_train == i)\n",
    "    ind_train = np.where(groups_train != i)\n",
    "    X = X_train[ind_train]\n",
    "    X_tt = X_train[ind_test]\n",
    "    y = y_train[ind_train]\n",
    "    y_tt = y_train[ind_test]\n",
    "    clf = xgb.XGBClassifier(objective='binary:logistic')\n",
    "    clf_lgb = lgb.train(params, lgb.Dataset(X, y))\n",
    "    clf_log = LogisticRegression(solver='lbfgs', max_iter=2000, C=10.0).fit(X, y)\n",
    "    clf_trees = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=0, n_jobs=-1).fit(X, y)\n",
    "    clf.fit(X, y)\n",
    "    my_pred = clf.predict_proba(X_tt)\n",
    "    my_pred_lgb = clf_lgb.predict(X_tt)\n",
    "    my_pred_log = clf_log.predict(X_tt)\n",
    "    my_pred_trees = clf_trees.predict(X_tt)\n",
    "    y_pred = my_pred[:, 1]\n",
    "    for t in range(30):\n",
    "        threshold = thres[t]\n",
    "        pred = np.empty_like(y_pred, dtype=bool)\n",
    "        pred[y_pred > threshold] = True\n",
    "        pred[y_pred <= threshold] = False\n",
    "        pred_lgb = my_pred_lgb > threshold\n",
    "        pred_log = my_pred_log > threshold\n",
    "        pred_trees = my_pred_trees > threshold\n",
    "        tres_lgb[i-1][t] = f1_score(y_tt, pred_lgb)\n",
    "        tres_log[i-1][t] = f1_score(y_tt, pred_log)\n",
    "        tres_trees[i-1][t] = f1_score(y_tt, pred_trees)\n",
    "        tres_scores[i-1][t] = f1_score(y_tt, pred)\n",
    "m = np.asarray(tres_scores).mean(axis=0)\n",
    "m_lgb = np.asarray(tres_lgb).mean(axis=0)\n",
    "m_log = np.asarray(tres_log).mean(axis=0)\n",
    "m_trees = np.asarray(tres_trees).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = lgb.train(params, lgb.Dataset(X_train, y_train)).predict(X_test) > 0.4\n",
    "pred2 = LogisticRegression(solver='lbfgs', max_iter=2000, C=20.0).fit(X_train, y_train).predict(X_test) > 0.4\n",
    "pred3 = xgb.XGBClassifier(objective='binary:logistic', base_score=0.4).fit(X_train, y_train).predict(X_test)\n",
    "pred4 = RandomForestClassifier().fit(X_train, y_train).predict(X_test) > 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1.sum(), pred2.sum(), pred3.sum(), pred4.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.array(pred1, dtype=int)\n",
    "pred2 = np.array(pred2, dtype=int)\n",
    "pred3 = np.array(pred3, dtype=int)\n",
    "pred4 = np.array(pred4, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = (pred1 + pred2 + pred3 + pred4) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = vote > 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred, dtype=np.int)\n",
    "with open('sample_submission.csv', 'w') as f:\n",
    "    fieldnames = ['pair_id', 'target']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    i = 11691\n",
    "    for elem in y_pred:\n",
    "        writer.writerow({'pair_id': str(i), 'target': str(elem)})\n",
    "        i += 1\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
